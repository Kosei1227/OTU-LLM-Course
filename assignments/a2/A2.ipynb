{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Neural Machine Translation\n",
    "\n",
    "This assignment will introduce the concept of neural networks and transformers, into the world of NLP. We will prepare a relevant dataset, preprocess it, split it into train, text and validation sets, build our own language vocabulary, a custom transformer model, and configure a training loop. Throughout this process, we will implement a standard machine learning pipeline and train a transformer model to perform neural machine translation (NMT). This workflow will establish foundational principles of natural language processing and allow us to work with state-of-the-art deep learning tools.\n",
    "\n",
    "This assignment will use the IWSLT (International Workshop on Spoken Language Translation) dataset. This dataset is a widely used and well-renowned parallel corpora consisting of various language pairs. It is great for our NMT task as it includes sentence-level aligned data, streamlining our traning and evaluation process. https://huggingface.co/datasets/IWSLT/iwslt2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "These are the tasks we will complete in this assignment:\n",
    "\n",
    "1. Data Preperation        \n",
    "2. Build Custom Vocabulary \n",
    "3. Custom Transformer Architecture\n",
    "4. Training Loop  \n",
    "5. Evaluation     \n",
    "6. Report Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install all required packages for this assignment into your current environment, follow the instructions below:\n",
    "\n",
    "```pip install -r requirements.txt```\n",
    "\n",
    "\n",
    "\n",
    "If you wish to create a new virtual environment, execute the following commands:\n",
    "\n",
    "```pip install virtualenv```\n",
    "```python -m venv {name your env}```\n",
    "\n",
    "e.g.\n",
    "\n",
    "```python -m venv myenv```\n",
    "\n",
    "This will create your environment folder. You can then:\n",
    "\n",
    "```{the name of your env}\\Scripts\\activate```\n",
    "\n",
    "e.g.\n",
    "\n",
    "```myenv\\Scripts\\activate```\n",
    "\n",
    "now:\n",
    "\n",
    "```pip install -r requirements.txt```\n",
    "\n",
    "You should now have all the required packages! If there were errors in installation of a package, you must resolve it otherwise it is possible none of the packages installed. Edit the requirements file if needed and remove lines causing issues, install those packages yourself to avoid version conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation\n",
    "Objective: Learn how to prepare data for training NMT models\n",
    "\n",
    "- In this task we will use the IWSLT dataset for language pairs of our choice. You can choose any available pairs you like\n",
    "- We will then preprocess the data through: tokenization, initialize symbols and special tokens, and converting text into numerical sequences\n",
    "- Split our dataset into training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "# Define the source and target languages of your choice\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Prepare tokenizer sets\n",
    "token_set = {}\n",
    "vocab_set = {}\n",
    "\n",
    "# Using torchtext's tokenizer\n",
    "token_set[SRC_LANGUAGE] = get_tokenizer('spacy', language='de')\n",
    "token_set[TGT_LANGUAGE] = get_tokenizer('spacy', language='en')\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "# Define the fields\n",
    "SRC = Field(tokenize=token_set[SRC_LANGUAGE], init_token='<bos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=token_set[TGT_LANGUAGE], init_token='<bos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "# Load the dataset\n",
    "train_data, valid_data, test_data = torchtext.legacy.datasets.IWSLT.splits(\n",
    "    exts=('.de', '.en'), fields=(SRC, TRG), root='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare data for training our NMT models. We will use the TorchText package for easy access to parallel texts from mutliple languages, including English, German, and French. It's a fantastic place for us to hit the ground running with multilingual machine translation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build Vocabulary\n",
    "\n",
    "Now we will implement a method to build a vocabulary from the training dataset. We will then convert sentences to sequences of token IDs using the custom vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "SRC.build_vocab(train_data.src, min_freq=2, specials=special_symbols)\n",
    "TRG.build_vocab(train_data.trg, min_freq=2, specials=special_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create iterators for efficient data handlign during training. We can leverage cuda GPU's if you have one available (you will need the CUDA version of pytorch installed), otherwise CPU is perfectly fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the iterators\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Custom Transformer Architecture\n",
    "\n",
    "We will now implement a custom transformer class using PyTorch modules and layers. Our implementation will include attention mechanisms, positional encoding, and feed-forward neural networks.\n",
    "\n",
    "We will also be implementing forward and masking methods for the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        src = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        trg = self.trg_embedding(trg) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        trg = self.pos_encoder(trg)\n",
    "        output = self.transformer(src, trg, src_mask, trg_mask)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.transformer.encoder(self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model)), src_mask)\n",
    "\n",
    "    def decode(self, trg, memory, trg_mask):\n",
    "        return self.transformer.decoder(self.pos_encoder(self.trg_embedding(trg) * math.sqrt(self.d_model)), memory, trg_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training Loop\n",
    "\n",
    "In this step, we will craft a typical deep learning training loop. We will:\n",
    "\n",
    "- Define our loss function and optimizer\n",
    "- Integrate gradient descent optimization, backpropagation, and loss computation\n",
    "- Train the transformer model and monitor the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_epoch(model, train_iterator, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in train_iterator:\n",
    "        src, trg = batch.src.to(device), batch.trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        src_mask = model.generate_square_subsequent_mask(src.size(0)).to(device)\n",
    "        trg_mask = model.generate_square_subsequent_mask(trg.size(0)).to(device)\n",
    "        \n",
    "        output = model(src, trg[:-1], src_mask, trg_mask)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(train_iterator)\n",
    "\n",
    "def evaluate(model, valid_iterator, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid_iterator:\n",
    "            src, trg = batch.src.to(device), batch.trg.to(device)\n",
    "            \n",
    "            src_mask = model.generate_square_subsequent_mask(src.size(0)).to(device)\n",
    "            trg_mask = model.generate_square_subsequent_mask(trg.size(0)).to(device)\n",
    "            \n",
    "            output = model(src, trg[:-1], src_mask, trg_mask)\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(valid_iterator)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Transformer(len(SRC.vocab), len(TRG.vocab), d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=SRC.vocab['<pad>'])\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, valid_iterator, criterion, device)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation\n",
    "\n",
    "Now that we have trained our model, we have to evaluate it! We will evaluate the model on the test set using industry-standard metrics such as BLEU score, analyze the translation quality, and discuss common translation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_bleu(data, model, SRC, TRG, device):\n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        pred_trg, _ = translate_sentence(src, SRC, TRG, model, device)\n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "    \n",
    "    return sentence_bleu(trgs, pred_trgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion and Reflection\n",
    "This is your chance to reflect on what has been learned and discuss potential real-world applications and further improvements.\n",
    "\n",
    "Please write a brief report discussing the experience. Include any challenges faced, summarize the process you went through (can be point-form and concise) and potential uses of the learned techniques in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
