{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpZd9V3wL-Vs"
      },
      "source": [
        "# Assignment 3: Adapting Languages with Fine-Tuning.\n",
        "\n",
        "This assignment guides students through the process of adapting existing language models to a low-resource language, providing hands-on experience with modern neural machine translation techniques and transfer learning strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikIAYfnVcWXD"
      },
      "source": [
        "In this notebook, we will dive into Yoruba-English.\n",
        "\n",
        "## MENYO-20k Dataset Summary\n",
        "\n",
        "The MENYO-20k dataset is a multi-domain parallel corpus designed for Yoruba-English neural machine translation (NMT). This dataset addresses the challenge of evaluating MT models on low-resource language pairs by providing a standardized evaluation set with clean orthography and diacritics. It includes texts from diverse domains such as news articles, TED talks, movie transcripts, and more. The dataset is publicly available under the CC BY-NC 4.0 license and is structured into training, development, and test splits for benchmarking.\n",
        "\n",
        "### Dataset Collection Sources\n",
        "\n",
        "| Source                         | Language Pair | No. Sentences |\n",
        "|-------------------------------|---------------|---------------|\n",
        "| Jehovah Witness News          | en-yo         | 3,508         |\n",
        "| Voice of Nigeria News         | en-yo         | 3,048         |\n",
        "| TED talks                     | en            | 2,945         |\n",
        "| Global Voices News            | en-yo         | 2,932         |\n",
        "| Yoruba Proverbs               | yo-en         | 2,700         |\n",
        "| Out of His Mind Book          | en            | 2,014         |\n",
        "| Software localization         | en            | 941           |\n",
        "| Movie Transcript (\"Unsane\")   | yo-en         | 774           |\n",
        "| Short texts                   | en            | 687           |\n",
        "| Radio Broadcast Transcript    | en            | 258           |\n",
        "| Creative Commons License      | en            | 193           |\n",
        "| UDHR Translation              | en-yo         | 100           |\n",
        "| **Total**                     |               | **20,100**    |\n",
        "\n",
        "### Domains and Train-Test Splits\n",
        "\n",
        "| Domain          | Training Set | Dev Set | Test Set |\n",
        "|-----------------|--------------|---------|----------|\n",
        "| News            | 4,995        | 1,391   | 3,102    |\n",
        "| TED Talks       | 507          | 438     | 2,000    |\n",
        "| Book            | -            | 1,006   | 1,008    |\n",
        "| IT              | 356          | 312     | 273      |\n",
        "| Yoruba Proverbs | 2,200        | 250     | 250      |\n",
        "| Others          | 2,012        | -       | -        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCPJEEqNesTL"
      },
      "source": [
        "## Dataset Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZbr5DG-hjZV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVwoCIshl0-9"
      },
      "outputs": [],
      "source": [
        "from jinja2 import Template\n",
        "from random import randint\n",
        "\n",
        "BASIC = \"Translate the following text from {{ s_lang }} to {{ t_lang }}: {{ s_text }}\"\n",
        "DESCRIPTIVE = \"Translate the following text from {{ s_lang }} to {{ t_lang }}. \\n\\n{{ s_lang }}: {{ s_text }} \\n\\n{{ t_lang }}:\"\n",
        "xP3 = \"{{ s_text }} the previous text is in {{ s_lang }}. Here is a translation to {{ t_lang }} \"\n",
        "\n",
        "TEMPLATES = [{\"template\": Template(DESCRIPTIVE), \"name\": \"descriptive\"}, {\"template\": Template(xP3), \"name\": \"xP3\"}]\n",
        "\n",
        "LANG_FLORES = {\n",
        "    \"afr\": \"afr_Latn\",\n",
        "    \"amh\": \"amh_Ethi\",\n",
        "    \"ara\": \"arz_Arab\",\n",
        "    \"eng\": \"eng_Latn\",\n",
        "    \"fra\": \"fra_Latn\",\n",
        "    \"hau\": \"hau_Latn\",\n",
        "    \"ibo\": \"ibo_Latn\",\n",
        "    \"kin\": \"kin_Latn\",\n",
        "    \"nya\": \"nya_Latn\",\n",
        "    \"por\": \"por_Latn\",\n",
        "    \"som\": \"som_Latn\",\n",
        "    \"sna\": \"sna_Latn\",\n",
        "    \"sot\": \"sot_Latn\",\n",
        "    \"swa\": \"swh_Latn\",\n",
        "    \"tir\": \"tir_Ethi\",\n",
        "    \"xho\": \"xho_Latn\",\n",
        "    \"yor\": \"yor_Latn\",\n",
        "    \"zul\": \"zul_Latn\",\n",
        "}\n",
        "\n",
        "LANG_NTREX = {\n",
        "    \"afr\": \"Afrikaans\",\n",
        "    \"amh\": \"Amharic\",\n",
        "    \"eng\": \"English\",\n",
        "    \"fra\": \"French\",\n",
        "    \"hau\": \"Hausa\",\n",
        "    \"ibo\": \"Igbo\",\n",
        "    \"kin\": \"Kinyarwanda\",\n",
        "    \"mlg\": \"Malagasy\",\n",
        "    \"nya\": \"Chichewa\",\n",
        "    \"orm\": \"Afaan Oromoo\",\n",
        "    \"por\": \"Portuguese\",\n",
        "    \"som\": \"Somali\",\n",
        "    \"sna\": \"Shona\",\n",
        "    \"swa\": \"Swahili\",\n",
        "    \"tir\": \"Tigrinya\",\n",
        "    \"xho\": \"Xhosa\",\n",
        "    \"yor\": \"Yoruba\",\n",
        "    \"zul\": \"Zulu\",\n",
        "}\n",
        "\n",
        "def get_prompt(s_lang, t_lang, s_text, t_text, s_code, t_code, source, split=\"train\"):\n",
        "\n",
        "    temp = randint(0, len(TEMPLATES) - 1)\n",
        "    prompt = TEMPLATES[temp]\n",
        "    is_reverse = randint(0, 1)\n",
        "\n",
        "    if is_reverse:\n",
        "            s_lang, t_lang = t_lang, s_lang\n",
        "            s_text, t_text = t_text, s_text\n",
        "            s_code, t_code = t_code, s_code\n",
        "\n",
        "    return {\n",
        "            \"instruction\": prompt[\"template\"].render(s_lang=s_lang, t_lang=t_lang, s_text=s_text),\n",
        "            \"output\": t_text,\n",
        "            \"lang\": f\"{s_code}-{t_code}\",\n",
        "            \"split\": split,\n",
        "            \"source\": source,\n",
        "            \"task\": \"translation\",\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxTtxJ4dKgGJ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "import os\n",
        "\n",
        "def get_dataset():\n",
        "    dataset = load_dataset(\"menyo20k_mt\")\n",
        "    result = []\n",
        "\n",
        "    for split in [\"train\", \"validation\", \"test\"]:\n",
        "        data = dataset[split]\n",
        "\n",
        "        for i in range(len(data)):\n",
        "            s_code = \"eng\"\n",
        "            t_code = \"yor\"\n",
        "            s_text = data[i][\"translation\"][\"en\"]\n",
        "            t_text = data[i][\"translation\"][\"yo\"]\n",
        "            s_lang = \"English\"\n",
        "            t_lang = \"Yoruba\"\n",
        "\n",
        "            result.append(get_prompt(s_lang, t_lang, s_text, t_text, s_code, t_code, \"MENYO\", split))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "def get__test_dataset(split):\n",
        "    dataset = load_dataset(\"menyo20k_mt\", split=split)\n",
        "    result = []\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        s_code = \"eng\"\n",
        "        t_code = \"yor\"\n",
        "        s_text = dataset[i][\"translation\"][\"en\"]\n",
        "        t_text = dataset[i][\"translation\"][\"yo\"]\n",
        "        s_lang = \"English\"\n",
        "        t_lang = \"Yoruba\"\n",
        "\n",
        "        result.append(get_prompt(s_lang, t_lang, s_text, t_text, s_code, t_code, \"MENYO\", split))\n",
        "\n",
        "\n",
        "    return result\n",
        "\n",
        "# Process the test split only\n",
        "test_data = get__test_dataset(\"test\")\n",
        "\n",
        "file_path = \"/content/data-test/MENYO_test_dataset.json\"\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "# Save the test dataset to a JSON file\n",
        "with open(file_path, \"w\") as f:\n",
        "    json.dump(test_data, f, ensure_ascii=False)\n",
        "\n",
        "file_path = \"/content/data-train/MENYO_dataset.json\"\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "with open(file_path, \"w\") as f:\n",
        "    json.dump(get_dataset(), f, ensure_ascii=False)\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_json(\"/content/data-train/MENYO_dataset.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YKYMYV7hd3S"
      },
      "outputs": [],
      "source": [
        "dataset.push_to_hub(\"YourAccountName/DatasetName\", token='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPDaXQg6ew7s"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSUB60dwe3b9"
      },
      "outputs": [],
      "source": [
        "model_name = \"llama-lang-adapt/pretrain-wura\"\n",
        "\n",
        "max_seq_length = 4096\n",
        "learning_rate = 1e-5\n",
        "weight_decay = 0.01\n",
        "max_steps = 500\n",
        "warmup_steps = 50\n",
        "batch_size = 2\n",
        "gradient_accumulation_steps = 4\n",
        "lr_scheduler_type = \"linear\"\n",
        "optimizer = \"adamw_8bit\"\n",
        "use_gradient_checkpointing = True\n",
        "random_state = 3407"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XchafgaVe7rt"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x11OByf_e9iB"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 4096\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "HAS_BFLOAT16 = torch.cuda.is_bf16_supported()\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR7LrvCifCVu"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Currently only supports dropout = 0\n",
        "    bias = \"none\",    # Currently only supports bias = \"none\"\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 3407,\n",
        "    max_seq_length = max_seq_length,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqsCVWADfGRk"
      },
      "outputs": [],
      "source": [
        "# @title Alpaca dataset preparation\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# # Example usage with a specific system prompt\n",
        "system_prompt = \"You are very proficient in Yoruba, and you are very good at responding in Yoruba.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYm5pDmVfMJc"
      },
      "outputs": [],
      "source": [
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func2(examples):\n",
        "    instruction = system_prompt\n",
        "    inputs       = examples[\"instruction\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for input, output in zip(inputs, outputs):\n",
        "        # text = alpaca_prompt.format(instruction, input, output)\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func2, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94E8FwsFfWyB"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from transformers.utils import logging\n",
        "logging.set_verbosity_info()\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    tokenizer = tokenizer,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = batch_size,\n",
        "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "        warmup_steps = warmup_steps,\n",
        "        max_steps = max_steps,\n",
        "        learning_rate = learning_rate,\n",
        "        fp16 = not HAS_BFLOAT16,\n",
        "        bf16 = HAS_BFLOAT16,\n",
        "        logging_steps = 1,\n",
        "        output_dir = \"outputs\",\n",
        "        optim = optimizer,\n",
        "        weight_decay = weight_decay,\n",
        "        lr_scheduler_type = lr_scheduler_type,\n",
        "        seed = random_state,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07fdqNjBfYy9"
      },
      "outputs": [],
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbP4u1o3fZS_"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmaABdnIfb4m"
      },
      "outputs": [],
      "source": [
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZeokZrNfdqV"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"You are very proficient in African languages, and you are very good at responding in those languages.\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ITZ20-cfgJp"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\"YourAccountName/DatasetName\", use_auth_token=True) # Online saving\n",
        "tokenizer.push_to_hub(\"YourAccountName/DatasetName\", use_auth_token=True) # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nm__dbuiUDf"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Rzb9Yn3iVZH"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import json\n",
        "import torch\n",
        "import os\n",
        "\n",
        "max_seq_length = 4096\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "HAS_BFLOAT16 = torch.cuda.is_bf16_supported()\n",
        "\n",
        "prefix = \"You are very proficient in African languages, and you are very good at responding in those languages.\"\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "def save_first_50_rows(file_path, output_file_path):\n",
        "    # Load prompts from a JSON file\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Extract the first 50 rows\n",
        "    first_50_rows = data[:50]\n",
        "\n",
        "    # Save the first 50 rows to a new JSON file\n",
        "    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
        "    with open(output_file_path, 'w') as f:\n",
        "        json.dump(first_50_rows, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"First 50 rows saved to {output_file_path}\")\n",
        "    return output_file_path\n",
        "\n",
        "\n",
        "def generate_responses_with_unsloth(file_path, output_dir, max_pairs=50):\n",
        "    # Load prompts from a JSON file\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Handle each generated output\n",
        "    results = []\n",
        "    for i, entry in enumerate(data[:max_pairs]):  # Process only the first max_pairs entries\n",
        "        prompt = entry['instruction']\n",
        "\n",
        "        inputs = tokenizer(\n",
        "        [\n",
        "            alpaca_prompt.format(\n",
        "                prefix,  # instruction\n",
        "                prompt,  # input\n",
        "                \"\",  # output - leave this blank for generation!\n",
        "            )\n",
        "        ], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
        "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        split_outputs = [text.split('### Response:') for text in decoded_outputs]\n",
        "        output = split_outputs[0][-1]\n",
        "        clean_output = output.replace(\"</s>\", \"\")\n",
        "\n",
        "        result = {\n",
        "            \"instruction\": prompt,\n",
        "            \"output\": clean_output,\n",
        "            \"lang\": entry.get('lang', 'N/A'),  # Handle optional fields\n",
        "            \"split\": entry.get('split', 'N/A'),\n",
        "            \"source\": entry.get('source', 'N/A'),\n",
        "            \"task\": entry.get('task', 'N/A')\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # Save results to a new JSON file\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_filename = os.path.join(output_dir, os.path.basename(file_path))\n",
        "    with open(output_filename, 'w') as f:\n",
        "        json.dump(results, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Inference complete and results saved for {file_path}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/data-test/MENYO_test_dataset.json\"\n",
        "output_dir = \"/content/inference\"\n",
        "first_50_file_path = \"/content/data-eval/MENYO_test_first_50.json\"\n",
        "\n",
        "first_50_file_path = save_first_50_rows(file_path, first_50_file_path)\n",
        "\n",
        "# Generate responses using the first 50 rows\n",
        "generate_responses_with_unsloth(first_50_file_path, output_dir, max_pairs=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5srLQGlUndoX"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R35SJKwznftU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import load_metric\n",
        "import os\n",
        "\n",
        "def load_benchmark(filepath):\n",
        "    with open(filepath, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "def load_predictions(filepath):\n",
        "    with open(filepath, 'r') as file:\n",
        "        predictions = json.load(file)\n",
        "    return predictions\n",
        "\n",
        "def evaluate_translation(benchmarks, predictions):\n",
        "    chrf = load_metric('chrf', trust_remote_code=True)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for bench, pred in zip(benchmarks, predictions):\n",
        "        if bench['task'] == 'translation':\n",
        "            ref = [[bench['output']]]\n",
        "            hypo = [pred['output']]\n",
        "            score = chrf.compute(predictions=hypo, references=ref)['score']\n",
        "            results.append(score)\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_translation_evaluation(benchmark_dir, prediction_dir, description):\n",
        "    total_scores = []\n",
        "\n",
        "    print(f\"Translation Evaluation Results for {description}:\")\n",
        "\n",
        "    benchmark_files = sorted([os.path.join(benchmark_dir, f) for f in os.listdir(benchmark_dir) if f.endswith('.json')])\n",
        "    prediction_files = sorted([os.path.join(prediction_dir, f) for f in os.listdir(prediction_dir) if f.endswith('.json')])\n",
        "\n",
        "    for benchmark_file, prediction_file in zip(benchmark_files, prediction_files):\n",
        "        benchmark_data = load_benchmark(benchmark_file)\n",
        "        predictions_data = load_predictions(prediction_file)\n",
        "        evaluation_results = evaluate_translation(benchmark_data, predictions_data)\n",
        "\n",
        "        average_score = sum(evaluation_results) / len(evaluation_results) if evaluation_results else 0\n",
        "        total_scores.append(average_score)\n",
        "\n",
        "        print(f\"{os.path.basename(benchmark_file)} - Average chrF Score: {average_score:.2f}\")\n",
        "\n",
        "    overall_average_score = sum(total_scores) / len(total_scores) if total_scores else 0\n",
        "    print(f\"Overall Average chrF Score for {description}: {overall_average_score:.2f}\\n\")\n",
        "\n",
        "# Example usage:\n",
        "benchmark_dir = \"/content/data-eval/\"\n",
        "prediction_dir = \"/content/inference/\"\n",
        "\n",
        "run_translation_evaluation(benchmark_dir, prediction_dir, \"MenYo Fine-tune\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
